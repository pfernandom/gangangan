{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise(batch_size):\n",
    "    return tf.random.normal([batch_size, input_shape[0], input_shape[1], input_shape[2]], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.disc = discriminator\n",
    "        self.gen = generator\n",
    "        self.gen_loss_tracker = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker, self.total_loss_tracker]\n",
    "\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "\n",
    "    def train_step(self, X, y):\n",
    "        noise_dim = 2\n",
    "        batch_size = tf.shape(X)[0]\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            noise = gen_noise(batch_size)\n",
    "            generated_output = self.gen(noise)\n",
    "\n",
    "            all_output = tf.concat([X, generated_output], axis=0)\n",
    "            all_labels = tf.concat([tf.ones(batch_size,1), tf.zeros(batch_size,1)], axis=0)\n",
    "\n",
    "            all_predictions = self.disc(all_output)\n",
    "\n",
    "            disc_loss = self.loss_fn(all_labels, all_predictions)\n",
    "\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.disc.trainable_variables)\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            noise = gen_noise(batch_size)\n",
    "            generated_output = self.gen(noise)\n",
    "\n",
    "            all_predictions = self.disc(generated_output)\n",
    "\n",
    "            all_labels = tf.ones(batch_size,1)\n",
    "\n",
    "            gen_loss = self.loss_fn(all_labels, all_predictions)\n",
    "\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
    "        \n",
    "\n",
    "        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.gen.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.disc.trainable_variables))\n",
    "\n",
    "        #Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(gen_loss)\n",
    "        self.disc_loss_tracker.update_state(disc_loss)\n",
    "        self.total_loss_tracker.update_state(gen_loss+disc_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "            \"d_total_loss\":  self.total_loss_tracker.result()\n",
    "        }\n",
    "\n",
    "gen = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.leaky_relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=64, activation=tf.nn.leaky_relu),\n",
    "    tf.keras.layers.Dense(units=2),\n",
    "],name=\"generator\")\n",
    "\n",
    "disc = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.leaky_relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=64, activation=tf.nn.leaky_relu),\n",
    "    tf.keras.layers.Dense(units=2),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "\n",
    "],name=\"discriminator\")\n",
    "\n",
    "cond_gan = GAN(\n",
    "    discriminator=disc, generator=gen\n",
    ")\n",
    "\n",
    "cond_gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    g_optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(train_images, train_labels, epochs=30, batch_size=64)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
